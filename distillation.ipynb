{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the knowledge distillation technique to train a YOLO v3 tiny model on the pedestrian detection task. We will use the already pretrained YOLO 8n model as the teacher model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, images_path, labels_path, transform=None):\n",
    "        self.images_path = images_path\n",
    "        self.labels_path = labels_path\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(images_path))\n",
    "        self.label_files = sorted(os.listdir(labels_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image_path = os.path.join(self.images_path, self.image_files[idx])\n",
    "        label_path = os.path.join(self.labels_path, self.label_files[idx])\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load labels\n",
    "        labels = []\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                class_id, x_center, y_center, width, height = map(float, line.split())\n",
    "                labels.append([class_id, x_center, y_center, width, height])\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # Apply transforms (if any)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Knowledge Distillation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, temperature=3.0):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        # Distillation loss (soft targets)\n",
    "        distillation_loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "            nn.functional.log_softmax(student_logits / self.temperature, dim=1),\n",
    "            nn.functional.softmax(teacher_logits / self.temperature, dim=1)\n",
    "        ) * (self.temperature ** 2)\n",
    "\n",
    "        # Standard cross-entropy loss (hard targets)\n",
    "        ce_loss = self.ce_loss(student_logits, labels)\n",
    "\n",
    "        # Combine both losses\n",
    "        return self.alpha * distillation_loss + (1 - self.alpha) * ce_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to dataset\n",
    "train_images = \"dataset/train/images\"\n",
    "train_labels = \"dataset/train/labels\"\n",
    "valid_images = \"dataset/valid/images\"\n",
    "valid_labels = \"dataset/valid/labels\"\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = YOLODataset(train_images, train_labels, transform=transform)\n",
    "valid_dataset = YOLODataset(valid_images, valid_labels, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: x)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, collate_fn=lambda x: x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load Teacher and Student Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): DetectionModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): Conv(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): Conv(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (11): ZeroPad2d((0, 1, 0, 1))\n",
       "      (12): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (13): Conv(\n",
       "        (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(1024, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (14): Conv(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (15): Conv(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (16): Conv(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (17): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (18): Concat()\n",
       "      (19): Conv(\n",
       "        (conv): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (20): Detect(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Load teacher model (YOLOv3-Tiny)\n",
    "# teacher_model = YOLO(\"yolov3-tiny.pt\")\n",
    "model = YOLO(\"yolov3-tinyu.pt\")\n",
    "torch.save(model, \"training_output/runs/detect/train/weights/yolov3-tinyu.pt\")\n",
    "\n",
    "teacher_model = torch.load(\"training_output/runs/detect/train/weights/best.pt\")\n",
    "\n",
    "# Load student model (YOLOv8n)\n",
    "student_model = torch.load(\"training_output/runs/detect/train/weights/yolov3-tinyu.pt\").to(device)\n",
    "student_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Define Optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(student_model.model.parameters(), lr=1e-4)\n",
    "criterion = DistillationLoss(alpha=0.7, temperature=3.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.49 available  Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov3-tinyu.pt, data=coco.yaml, epochs=100, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train10, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=D:\\Epita\\MLOps\\Pedestrian_Detection_RPI\\runs\\detect\\train10\n",
      "\n",
      "Dataset 'coco.yaml' images not found , missing path 'D:\\Epita\\MLOps\\datasets\\coco\\val2017.txt'\n",
      "WARNING  Skipping D:\\Epita\\MLOps\\datasets\\coco2017labels-segments.zip unzip as destination directory D:\\Epita\\MLOps\\datasets\\coco is not empty.\n",
      "Downloading http://images.cocodataset.org/zips/train2017.zip to 'D:\\Epita\\MLOps\\datasets\\coco\\images\\train2017.zip'...\n",
      "Downloading http://images.cocodataset.org/zips/val2017.zip to 'D:\\Epita\\MLOps\\datasets\\coco\\images\\val2017.zip'...\n",
      "Downloading http://images.cocodataset.org/zips/test2017.zip to 'D:\\Epita\\MLOps\\datasets\\coco\\images\\test2017.zip'...\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # student_model.train()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch\n",
    "\n",
    "        # Move to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Teacher predictions (detach to avoid gradients)\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model.model(images)\n",
    "\n",
    "        # Student predictions\n",
    "        student_outputs = student_model.model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(student_outputs, teacher_outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    student_model.eval()  # Passer le modèle en mode évaluation\n",
    "\n",
    "    validation_loss = 0.0\n",
    "    num_batches = len(valid_loader)\n",
    "\n",
    "    with torch.no_grad():  # Pas de calcul de gradients pendant la validation\n",
    "        for batch in valid_loader:\n",
    "            images, labels = batch\n",
    "\n",
    "            # Déplacer les données vers le device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Prédictions du modèle étudiant (student)\n",
    "            student_outputs = student_model.model(images)\n",
    "\n",
    "            # Calcul de la loss\n",
    "            loss = criterion(student_outputs, student_outputs, labels)  # Ici, vous pouvez ajuster selon la distillation\n",
    "\n",
    "            # Accumuler la loss pour calculer la moyenne\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "    # Calcul de la loss moyenne sur l'ensemble du set de validation\n",
    "    average_validation_loss = validation_loss / num_batches\n",
    "    print(f\"Validation Loss: {average_validation_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in valid_loader:\n",
    "        images, labels = batch\n",
    "\n",
    "        images = images.to(device)\n",
    "        predictions = student_model(images)\n",
    "\n",
    "        # Evaluate predictions (e.g., mAP or custom evaluation)\n",
    "        print(\"Validation predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Save the Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.save(\"yolov8n_distilled.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
